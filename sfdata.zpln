{
  "paragraphs": [
    {
      "text": "%pyspark\nspark",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T18:42:28+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<pyspark.sql.session.SparkSession object at 0x7fd26bfc6810>\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681552795021_1092995070",
      "id": "paragraph_1681552795021_1092995070",
      "dateCreated": "2023-04-15T09:59:55+0000",
      "dateStarted": "2023-04-19T18:42:28+0000",
      "dateFinished": "2023-04-19T18:43:07+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:8483"
    },
    {
      "text": "%sh\nls data\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T17:55:00+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "batches\nfire.csv\ngrades.csv\nssn-address.tsv\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681609944522_1722959828",
      "id": "paragraph_1681609944522_1722959828",
      "dateCreated": "2023-04-16T01:52:24+0000",
      "dateStarted": "2023-04-19T17:55:00+0000",
      "dateFinished": "2023-04-19T17:55:01+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8484"
    },
    {
      "text": "%pyspark\nfile_loc=\"file:///data/fire.csv\"\nheader1 = \"\"\"call_number,unit_id,incident_number,call_type,call_date,watch_date,received_dttm,entry_dttm,dispatch_dttm,response_dttm,on_scene_dttm,transport_dttm,hospital_dttm,call_final_disposition,available_dttm,address,city,zipcode_of_Incident,battalion,station_area,box,original_priority,priority,final_priority,als_unit,call_type_group,number_of_alarms,unit_type,unit_sequence_in_call_dispatch,fire_prevention_district,supervisor _district,neighborhooods_analysis_boundaries,row_id,case_location,analysis_neighborhoods\"\"\"\nheader2 = header1.split(\",\")\ntype(header2)\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T18:51:41+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<class 'list'>\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681926321878_463859216",
      "id": "paragraph_1681926321878_463859216",
      "dateCreated": "2023-04-19T17:45:21+0000",
      "dateStarted": "2023-04-19T18:51:41+0000",
      "dateFinished": "2023-04-19T18:51:41+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8485"
    },
    {
      "text": "%pyspark\nrdd = sc.textFile(file_loc)\nrdd.cache()\nheader = rdd.first()\ndata = rdd.filter(lambda a:a!=header).map(lambda a:a.split(\",\"))",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T18:58:56+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=23",
              "$$hashKey": "object:9446"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681926310894_1030559844",
      "id": "paragraph_1681926310894_1030559844",
      "dateCreated": "2023-04-19T17:45:10+0000",
      "dateStarted": "2023-04-19T18:58:56+0000",
      "dateFinished": "2023-04-19T18:58:57+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8486"
    },
    {
      "text": "%pyspark\ntype(header)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T18:54:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<class 'list'>\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681930360189_646145971",
      "id": "paragraph_1681930360189_646145971",
      "dateCreated": "2023-04-19T18:52:40+0000",
      "dateStarted": "2023-04-19T18:54:15+0000",
      "dateFinished": "2023-04-19T18:54:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8487"
    },
    {
      "text": "%pyspark\nrdd_df = data.toDF(header.split(\",\"))",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T18:59:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=24",
              "$$hashKey": "object:9546"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681930461447_1744707277",
      "id": "paragraph_1681930461447_1744707277",
      "dateCreated": "2023-04-19T18:54:21+0000",
      "dateStarted": "2023-04-19T18:59:11+0000",
      "dateFinished": "2023-04-19T18:59:12+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8488"
    },
    {
      "text": "%pyspark\nrdd_df.show(2,vertical = True)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T18:59:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-RECORD 0----------------------------------------------------\n Call Number                          | 221210313            \n Unit ID                              | E36                  \n Incident Number                      | 22054955             \n Call Type                            | Outside Fire         \n Call Date                            | 05/01/2022           \n Watch Date                           | 04/30/2022           \n Received DtTm                        | 05/01/2022 02:58:... \n Entry DtTm                           | 05/01/2022 02:59:... \n Dispatch DtTm                        | 05/01/2022 02:59:... \n Response DtTm                        | 05/01/2022 03:01:... \n On Scene DtTm                        | 05/01/2022 03:02:... \n Transport DtTm                       |                      \n Hospital DtTm                        |                      \n Call Final Disposition               | Fire                 \n Available DtTm                       | 05/01/2022 03:05:... \n Address                              | GOUGH ST/GROVE ST    \n City                                 | San Francisco        \n Zipcode of Incident                  | 94102                \n Battalion                            | B02                  \n Station Area                         | 36                   \n Box                                  | 3265                 \n Original Priority                    | 3                    \n Priority                             | 3                    \n Final Priority                       | 3                    \n ALS Unit                             | true                 \n Call Type Group                      | Fire                 \n Number of Alarms                     | 1                    \n Unit Type                            | ENGINE               \n Unit sequence in call dispatch       | 1                    \n Fire Prevention District             | 2                    \n Supervisor District                  | 5                    \n Neighborhooods - Analysis Boundaries | Hayes Valley         \n RowID                                | 221210313-E36        \n case_location                        | POINT (-122.42316... \n Analysis Neighborhoods               | 9                    \n-RECORD 1----------------------------------------------------\n Call Number                          | 220190150            \n Unit ID                              | E29                  \n Incident Number                      | 22008871             \n Call Type                            | Alarms               \n Call Date                            | 01/19/2022           \n Watch Date                           | 01/18/2022           \n Received DtTm                        | 01/19/2022 01:42:... \n Entry DtTm                           | 01/19/2022 01:44:... \n Dispatch DtTm                        | 01/19/2022 01:44:... \n Response DtTm                        | 01/19/2022 01:46:... \n On Scene DtTm                        | 01/19/2022 01:49:... \n Transport DtTm                       |                      \n Hospital DtTm                        |                      \n Call Final Disposition               | Fire                 \n Available DtTm                       | 01/19/2022 02:35:... \n Address                              | 100 Block of MISS... \n City                                 | San Francisco        \n Zipcode of Incident                  | 94107                \n Battalion                            | B03                  \n Station Area                         | 29                   \n Box                                  | 2431                 \n Original Priority                    | 3                    \n Priority                             | 3                    \n Final Priority                       | 3                    \n ALS Unit                             | true                 \n Call Type Group                      | Alarm                \n Number of Alarms                     | 1                    \n Unit Type                            | ENGINE               \n Unit sequence in call dispatch       | 1                    \n Fire Prevention District             | 3                    \n Supervisor District                  | 10                   \n Neighborhooods - Analysis Boundaries | Potrero Hill         \n RowID                                | 220190150-E29        \n case_location                        | POINT (-122.39469... \n Analysis Neighborhoods               | 26                   \nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=25",
              "$$hashKey": "object:9604"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681930495610_2003838205",
      "id": "paragraph_1681930495610_2003838205",
      "dateCreated": "2023-04-19T18:54:55+0000",
      "dateStarted": "2023-04-19T18:59:20+0000",
      "dateFinished": "2023-04-19T18:59:20+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8489"
    },
    {
      "text": "%pyspark\nrdd_df2= data.toDF(header2)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T18:59:26+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=26",
              "$$hashKey": "object:9652"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681928868833_465196497",
      "id": "paragraph_1681928868833_465196497",
      "dateCreated": "2023-04-19T18:27:48+0000",
      "dateStarted": "2023-04-19T18:59:26+0000",
      "dateFinished": "2023-04-19T18:59:27+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8490"
    },
    {
      "text": "%pyspark\nrdd_df2.show(2,truncate = False, vertical = True)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T19:52:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-RECORD 0---------------------------------------------------------------------------\n call_number                        | 221210313                                     \n unit_id                            | E36                                           \n incident_number                    | 22054955                                      \n call_type                          | Outside Fire                                  \n call_date                          | 05/01/2022                                    \n watch_date                         | 04/30/2022                                    \n received_dttm                      | 05/01/2022 02:58:25 AM                        \n entry_dttm                         | 05/01/2022 02:59:15 AM                        \n dispatch_dttm                      | 05/01/2022 02:59:25 AM                        \n response_dttm                      | 05/01/2022 03:01:06 AM                        \n on_scene_dttm                      | 05/01/2022 03:02:27 AM                        \n transport_dttm                     |                                               \n hospital_dttm                      |                                               \n call_final_disposition             | Fire                                          \n available_dttm                     | 05/01/2022 03:05:00 AM                        \n address                            | GOUGH ST/GROVE ST                             \n city                               | San Francisco                                 \n zipcode_of_Incident                | 94102                                         \n battalion                          | B02                                           \n station_area                       | 36                                            \n box                                | 3265                                          \n original_priority                  | 3                                             \n priority                           | 3                                             \n final_priority                     | 3                                             \n als_unit                           | true                                          \n call_type_group                    | Fire                                          \n number_of_alarms                   | 1                                             \n unit_type                          | ENGINE                                        \n unit_sequence_in_call_dispatch     | 1                                             \n fire_prevention_district           | 2                                             \n supervisor _district               | 5                                             \n neighborhooods_analysis_boundaries | Hayes Valley                                  \n row_id                             | 221210313-E36                                 \n case_location                      | POINT (-122.42316555403964 37.77781524520032) \n analysis_neighborhoods             | 9                                             \n-RECORD 1---------------------------------------------------------------------------\n call_number                        | 220190150                                     \n unit_id                            | E29                                           \n incident_number                    | 22008871                                      \n call_type                          | Alarms                                        \n call_date                          | 01/19/2022                                    \n watch_date                         | 01/18/2022                                    \n received_dttm                      | 01/19/2022 01:42:12 AM                        \n entry_dttm                         | 01/19/2022 01:44:13 AM                        \n dispatch_dttm                      | 01/19/2022 01:44:28 AM                        \n response_dttm                      | 01/19/2022 01:46:47 AM                        \n on_scene_dttm                      | 01/19/2022 01:49:32 AM                        \n transport_dttm                     |                                               \n hospital_dttm                      |                                               \n call_final_disposition             | Fire                                          \n available_dttm                     | 01/19/2022 02:35:26 AM                        \n address                            | 100 Block of MISSISSIPPI ST                   \n city                               | San Francisco                                 \n zipcode_of_Incident                | 94107                                         \n battalion                          | B03                                           \n station_area                       | 29                                            \n box                                | 2431                                          \n original_priority                  | 3                                             \n priority                           | 3                                             \n final_priority                     | 3                                             \n als_unit                           | true                                          \n call_type_group                    | Alarm                                         \n number_of_alarms                   | 1                                             \n unit_type                          | ENGINE                                        \n unit_sequence_in_call_dispatch     | 1                                             \n fire_prevention_district           | 3                                             \n supervisor _district               | 10                                            \n neighborhooods_analysis_boundaries | Potrero Hill                                  \n row_id                             | 220190150-E29                                 \n case_location                      | POINT (-122.39469970274361 37.76460987856451) \n analysis_neighborhoods             | 26                                            \nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=36",
              "$$hashKey": "object:9710"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681930668412_1397931890",
      "id": "paragraph_1681930668412_1397931890",
      "dateCreated": "2023-04-19T18:57:48+0000",
      "dateStarted": "2023-04-19T19:52:11+0000",
      "dateFinished": "2023-04-19T19:52:11+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8491"
    },
    {
      "text": "%pyspark\nrdd_df2.printSchema()",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T19:10:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- call_number: string (nullable = true)\n |-- unit_id: string (nullable = true)\n |-- incident_number: string (nullable = true)\n |-- call_type: string (nullable = true)\n |-- call_date: string (nullable = true)\n |-- watch_date: string (nullable = true)\n |-- received_dttm: string (nullable = true)\n |-- entry_dttm: string (nullable = true)\n |-- dispatch_dttm: string (nullable = true)\n |-- response_dttm: string (nullable = true)\n |-- on_scene_dttm: string (nullable = true)\n |-- transport_dttm: string (nullable = true)\n |-- hospital_dttm: string (nullable = true)\n |-- call_final_disposition: string (nullable = true)\n |-- available_dttm: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- zipcode_of_Incident: string (nullable = true)\n |-- battalion: string (nullable = true)\n |-- station_area: string (nullable = true)\n |-- box: string (nullable = true)\n |-- original_priority: string (nullable = true)\n |-- priority: string (nullable = true)\n |-- final_priority: string (nullable = true)\n |-- als_unit: string (nullable = true)\n |-- call_type_group: string (nullable = true)\n |-- number_of_alarms: string (nullable = true)\n |-- unit_type: string (nullable = true)\n |-- unit_sequence_in_call_dispatch: string (nullable = true)\n |-- fire_prevention_district: string (nullable = true)\n |-- supervisor _district: string (nullable = true)\n |-- neighborhooods_analysis_boundaries: string (nullable = true)\n |-- row_id: string (nullable = true)\n |-- case_location: string (nullable = true)\n |-- analysis_neighborhoods: string (nullable = true)\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681931391389_237082335",
      "id": "paragraph_1681931391389_237082335",
      "dateCreated": "2023-04-19T19:09:51+0000",
      "dateStarted": "2023-04-19T19:10:15+0000",
      "dateFinished": "2023-04-19T19:10:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8492"
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import col, to_date, to_timestamp\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType,DateType,TimestampType,BooleanType\n\nfile_loc=\"file:///data/fire.csv\"\nheader1 = \"\"\"call_number,unit_id,incident_number,call_type,call_date,watch_date,received_dttm,entry_dttm,dispatch_dttm,response_dttm,on_scene_dttm,transport_dttm,hospital_dttm,call_final_disposition,available_dttm,address,city,zipcode_of_Incident,battalion,station_area,box,original_priority,priority,final_priority,als_unit,call_type_group,number_of_alarms,unit_type,unit_sequence_in_call_dispatch,fire_prevention_district,supervisor _district,neighborhooods_analysis_boundaries,row_id,case_location,analysis_neighborhoods\"\"\"\n\nheader2 = header1.split(\",\")\n\n\nrdd = sc.textFile(file_loc)\nrdd.cache()\nheader = rdd.first()\ndata = rdd.filter(lambda a:a!=header).map(lambda a:a.split(\",\"))\nrdd_df2= data.toDF(header2)\nfinal_df = rdd_df2.withColumn('call_date',to_date(col('call_date'),\"dd/mm/yyyy\")) .\\\nwithColumn('call_number',col('call_number').cast(IntegerType())). \\\nwithColumn('incident_number',col('incident_number').cast(IntegerType())). \\\nwithColumn('watch_date',to_date(col('watch_date'),\"dd/mm/yyyy\")). \\\nwithColumn('received_dttm',to_timestamp(col('received_dttm'),\"dd/MM/yyyy HH:mm:ss\")). \\\nwithColumn('entry_dttm',to_timestamp(col('entry_dttm'),\"dd/MM/yyyy HH:mm:ss\")).\\\nwithColumn('dispatch_dttm',to_timestamp(col('dispatch_dttm'),\"dd/MM/yyyy HH:mm:ss\")). \\\nwithColumn('response_dttm',to_timestamp(col('response_dttm'),\"dd/MM/yyyy HH:mm:ss\")).\\\nwithColumn('on_scene_dttm',to_timestamp(col('on_scene_dttm'),\"dd/MM/yyyy HH:mm:ss\")). \\\nwithColumn('transport_dttm',to_timestamp(col('transport_dttm'),\"dd/MM/yyyy HH:mm:ss\")).\\\nwithColumn('available_dttm',to_timestamp(col('available_dttm'),\"dd/MM/yyyy HH:mm:ss\")).\\\nwithColumn('hospital_dttm',to_timestamp(col('hospital_dttm'),\"dd/MM/yyyy HH:mm:ss\"))\n\nfinal_df.printSchema()\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T20:20:02+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- call_number: integer (nullable = true)\n |-- unit_id: string (nullable = true)\n |-- incident_number: integer (nullable = true)\n |-- call_type: string (nullable = true)\n |-- call_date: date (nullable = true)\n |-- watch_date: date (nullable = true)\n |-- received_dttm: timestamp (nullable = true)\n |-- entry_dttm: timestamp (nullable = true)\n |-- dispatch_dttm: timestamp (nullable = true)\n |-- response_dttm: timestamp (nullable = true)\n |-- on_scene_dttm: timestamp (nullable = true)\n |-- transport_dttm: timestamp (nullable = true)\n |-- hospital_dttm: timestamp (nullable = true)\n |-- call_final_disposition: string (nullable = true)\n |-- available_dttm: timestamp (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- zipcode_of_Incident: string (nullable = true)\n |-- battalion: string (nullable = true)\n |-- station_area: string (nullable = true)\n |-- box: string (nullable = true)\n |-- original_priority: string (nullable = true)\n |-- priority: string (nullable = true)\n |-- final_priority: string (nullable = true)\n |-- als_unit: string (nullable = true)\n |-- call_type_group: string (nullable = true)\n |-- number_of_alarms: string (nullable = true)\n |-- unit_type: string (nullable = true)\n |-- unit_sequence_in_call_dispatch: string (nullable = true)\n |-- fire_prevention_district: string (nullable = true)\n |-- supervisor _district: string (nullable = true)\n |-- neighborhooods_analysis_boundaries: string (nullable = true)\n |-- row_id: string (nullable = true)\n |-- case_location: string (nullable = true)\n |-- analysis_neighborhoods: string (nullable = true)\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=50",
              "$$hashKey": "object:9820"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=51",
              "$$hashKey": "object:9821"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681932315669_1905377016",
      "id": "paragraph_1681932315669_1905377016",
      "dateCreated": "2023-04-19T19:25:15+0000",
      "dateStarted": "2023-04-19T20:20:02+0000",
      "dateFinished": "2023-04-19T20:20:09+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8493"
    },
    {
      "text": "%pyspark\nfinal_df.show(2,vertical =True,truncate = False)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T20:12:14+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-RECORD 0---------------------------------------------------------------------------\n call_number                        | 221210313                                     \n unit_id                            | E36                                           \n incident_number                    | 22054955                                      \n call_type                          | Outside Fire                                  \n call_date                          | 2022-01-05                                    \n watch_date                         | 2022-01-04                                    \n received_dttm                      | 2022-01-05 02:58:25                           \n entry_dttm                         | 2022-01-05 02:59:15                           \n dispatch_dttm                      | 2022-01-05 02:59:25                           \n response_dttm                      | 2022-01-05 03:01:06                           \n on_scene_dttm                      | 2022-01-05 03:02:27                           \n transport_dttm                     | null                                          \n hospital_dttm                      | null                                          \n call_final_disposition             | Fire                                          \n available_dttm                     | 2022-01-05 03:05:00                           \n address                            | GOUGH ST/GROVE ST                             \n city                               | San Francisco                                 \n zipcode_of_Incident                | 94102                                         \n battalion                          | B02                                           \n station_area                       | 36                                            \n box                                | 3265                                          \n original_priority                  | 3                                             \n priority                           | 3                                             \n final_priority                     | 3                                             \n als_unit                           | true                                          \n call_type_group                    | Fire                                          \n number_of_alarms                   | 1                                             \n unit_type                          | ENGINE                                        \n unit_sequence_in_call_dispatch     | 1                                             \n fire_prevention_district           | 2                                             \n supervisor _district               | 5                                             \n neighborhooods_analysis_boundaries | Hayes Valley                                  \n row_id                             | 221210313-E36                                 \n case_location                      | POINT (-122.42316555403964 37.77781524520032) \n analysis_neighborhoods             | 9                                             \n-RECORD 1---------------------------------------------------------------------------\n call_number                        | 220190150                                     \n unit_id                            | E29                                           \n incident_number                    | 22008871                                      \n call_type                          | Alarms                                        \n call_date                          | 2022-01-01                                    \n watch_date                         | 2022-01-01                                    \n received_dttm                      | null                                          \n entry_dttm                         | null                                          \n dispatch_dttm                      | null                                          \n response_dttm                      | null                                          \n on_scene_dttm                      | null                                          \n transport_dttm                     | null                                          \n hospital_dttm                      | null                                          \n call_final_disposition             | Fire                                          \n available_dttm                     | null                                          \n address                            | 100 Block of MISSISSIPPI ST                   \n city                               | San Francisco                                 \n zipcode_of_Incident                | 94107                                         \n battalion                          | B03                                           \n station_area                       | 29                                            \n box                                | 2431                                          \n original_priority                  | 3                                             \n priority                           | 3                                             \n final_priority                     | 3                                             \n als_unit                           | true                                          \n call_type_group                    | Alarm                                         \n number_of_alarms                   | 1                                             \n unit_type                          | ENGINE                                        \n unit_sequence_in_call_dispatch     | 1                                             \n fire_prevention_district           | 3                                             \n supervisor _district               | 10                                            \n neighborhooods_analysis_boundaries | Potrero Hill                                  \n row_id                             | 220190150-E29                                 \n case_location                      | POINT (-122.39469970274361 37.76460987856451) \n analysis_neighborhoods             | 26                                            \nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=48",
              "$$hashKey": "object:9883"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681932580551_1019946773",
      "id": "paragraph_1681932580551_1019946773",
      "dateCreated": "2023-04-19T19:29:40+0000",
      "dateStarted": "2023-04-19T20:12:14+0000",
      "dateFinished": "2023-04-19T20:12:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8494"
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType,DateType,TimestampType,BooleanType\n\nmy_schema = StructType([\n                                StructField(\"call_number\", IntegerType(), True),\n                                StructField(\"unit_id\",StringType(),True),\n                                StructField(\"incident_number\", IntegerType(),True),\n                                StructField(\"call_type\",StringType(),True),\n                                StructField(\"call_date\",DateType(),True),\n                                StructField(\"watch_date\",DateType(),True)  ,\n                                StructField(\"received_dtm\",TimestampType(),True),\n                                StructField(\"entry_dtm\",TimestampType(),True),\n                                StructField(\"dispatch_dtm\",TimestampType(),True),\n                                StructField(\"response_dtm\",TimestampType(),True),\n                                StructField(\"on_scene_dtm\",TimestampType(),True),\n                                StructField(\"transport_dtm\",TimestampType(),True),\n                                StructField(\"hospital_dtm\",TimestampType(),True),\n                                StructField(\"call_final_disposition\",StringType(),True),\n                                StructField(\"available_dtm\",TimestampType(),True),\n                                StructField(\"address\",StringType(),True),\n                                StructField(\"city\",StringType(),True),\n                                StructField(\"zipcode\",StringType(),True),\n                                StructField(\"battalion\",StringType(),True),\n                                StructField(\"station_area\",IntegerType(),True),\n                                StructField(\"box\",IntegerType(),True),\n                                StructField(\"original_priority\",IntegerType(),True),\n                                StructField(\"priority\",IntegerType(), True),\n                                StructField(\"final_priority\",IntegerType(),True),\n                                StructField(\"als_unit\",BooleanType(),True),\n                                StructField(\"call_type_group\",StringType(),True),\n                                StructField(\"number_of_alarms\",IntegerType(),True),\n                                StructField(\"unit_type\",StringType(),True),\n                                StructField(\"unit_sequence\",IntegerType(),True),\n                                StructField(\"Fire_prevention_district\",IntegerType(),True),\n                                StructField(\"supervisor_district\",IntegerType(),True),\n                                StructField(\"Neighborhoods\",StringType(),True),\n                                StructField(\"row_id\",StringType(),True),\n                                StructField(\"case_location\",StringType(),True),\n                                StructField(\"analysis_neighborhoods\",IntegerType(),True)  ] )",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T20:13:27+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681932146973_817543047",
      "id": "paragraph_1681932146973_817543047",
      "dateCreated": "2023-04-19T19:22:26+0000",
      "dateStarted": "2023-04-19T20:13:27+0000",
      "dateFinished": "2023-04-19T20:13:27+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8495"
    },
    {
      "text": "%pyspark\nrdd_df_with_schema= data.toDF(my_schema)\nrdd_df_with_schema.show(2,vertical = True)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T19:22:59+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o933.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 4 times, most recent failure: Lost task 0.3 in stage 32.0 (TID 44, worker2, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/spark/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1383, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field call_date: DateType can not accept object '05/01/2022' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/spark/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1383, in verify_default\n    verify_acceptable_types(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field call_date: DateType can not accept object '05/01/2022' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o933.showString.\\n', JavaObject id=o934), <traceback object at 0x7fd26a682190>)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=32",
              "$$hashKey": "object:9983"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681931099717_1444083498",
      "id": "paragraph_1681931099717_1444083498",
      "dateCreated": "2023-04-19T19:04:59+0000",
      "dateStarted": "2023-04-19T19:23:00+0000",
      "dateFinished": "2023-04-19T19:23:01+0000",
      "status": "ERROR",
      "$$hashKey": "object:8496"
    },
    {
      "text": "%pyspark\nrdd_df_with_schema.show(2,vertical = True)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T19:21:32+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o861.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 4 times, most recent failure: Lost task 0.3 in stage 31.0 (TID 40, worker2, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/spark/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field incident_number: IntegerType can not accept object '22054955' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/spark/python/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1370, in verify_struct\n    verifier(v)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1315, in verify_integer\n    verify_acceptable_types(obj)\n  File \"/usr/spark/python/pyspark/sql/types.py\", line 1278, in verify_acceptable_types\n    % (dataType, obj, type(obj))))\nTypeError: field incident_number: IntegerType can not accept object '22054955' in type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o861.showString.\\n', JavaObject id=o866), <traceback object at 0x7fd26a67dcd0>)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=31",
              "$$hashKey": "object:10041"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681931222941_768150752",
      "id": "paragraph_1681931222941_768150752",
      "dateCreated": "2023-04-19T19:07:02+0000",
      "dateStarted": "2023-04-19T19:21:32+0000",
      "dateFinished": "2023-04-19T19:21:33+0000",
      "status": "ERROR",
      "$$hashKey": "object:8497"
    },
    {
      "text": "%pyspark\ndf = spark.read.format(\"csv\").option(\"inferSchema\",False).schema(my_schema).option(\"header\",True).load(file_loc)\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T20:14:43+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681553256091_356668214",
      "id": "paragraph_1681553256091_356668214",
      "dateCreated": "2023-04-15T10:07:36+0000",
      "dateStarted": "2023-04-19T20:14:43+0000",
      "dateFinished": "2023-04-19T20:14:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8498"
    },
    {
      "text": "%pyspark\ndf.printSchema()\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T20:14:45+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- call_number: integer (nullable = true)\n |-- unit_id: string (nullable = true)\n |-- incident_number: integer (nullable = true)\n |-- call_type: string (nullable = true)\n |-- call_date: date (nullable = true)\n |-- watch_date: date (nullable = true)\n |-- received_dtm: timestamp (nullable = true)\n |-- entry_dtm: timestamp (nullable = true)\n |-- dispatch_dtm: timestamp (nullable = true)\n |-- response_dtm: timestamp (nullable = true)\n |-- on_scene_dtm: timestamp (nullable = true)\n |-- transport_dtm: timestamp (nullable = true)\n |-- hospital_dtm: timestamp (nullable = true)\n |-- call_final_disposition: string (nullable = true)\n |-- available_dtm: timestamp (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- battalion: string (nullable = true)\n |-- station_area: integer (nullable = true)\n |-- box: integer (nullable = true)\n |-- original_priority: integer (nullable = true)\n |-- priority: integer (nullable = true)\n |-- final_priority: integer (nullable = true)\n |-- als_unit: boolean (nullable = true)\n |-- call_type_group: string (nullable = true)\n |-- number_of_alarms: integer (nullable = true)\n |-- unit_type: string (nullable = true)\n |-- unit_sequence: integer (nullable = true)\n |-- Fire_prevention_district: integer (nullable = true)\n |-- supervisor_district: integer (nullable = true)\n |-- Neighborhoods: string (nullable = true)\n |-- row_id: string (nullable = true)\n |-- case_location: string (nullable = true)\n |-- analysis_neighborhoods: integer (nullable = true)\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681561758663_526799959",
      "id": "paragraph_1681561758663_526799959",
      "dateCreated": "2023-04-15T12:29:18+0000",
      "dateStarted": "2023-04-19T20:14:45+0000",
      "dateFinished": "2023-04-19T20:14:45+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8499"
    },
    {
      "text": "%pyspark\ndf.show(2,vertical=True)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T20:14:54+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-RECORD 0------------------------\n call_number              | null \n unit_id                  | null \n incident_number          | null \n call_type                | null \n call_date                | null \n watch_date               | null \n received_dtm             | null \n entry_dtm                | null \n dispatch_dtm             | null \n response_dtm             | null \n on_scene_dtm             | null \n transport_dtm            | null \n hospital_dtm             | null \n call_final_disposition   | null \n available_dtm            | null \n address                  | null \n city                     | null \n zipcode                  | null \n battalion                | null \n station_area             | null \n box                      | null \n original_priority        | null \n priority                 | null \n final_priority           | null \n als_unit                 | null \n call_type_group          | null \n number_of_alarms         | null \n unit_type                | null \n unit_sequence            | null \n Fire_prevention_district | null \n supervisor_district      | null \n Neighborhoods            | null \n row_id                   | null \n case_location            | null \n analysis_neighborhoods   | null \n-RECORD 1------------------------\n call_number              | null \n unit_id                  | null \n incident_number          | null \n call_type                | null \n call_date                | null \n watch_date               | null \n received_dtm             | null \n entry_dtm                | null \n dispatch_dtm             | null \n response_dtm             | null \n on_scene_dtm             | null \n transport_dtm            | null \n hospital_dtm             | null \n call_final_disposition   | null \n available_dtm            | null \n address                  | null \n city                     | null \n zipcode                  | null \n battalion                | null \n station_area             | null \n box                      | null \n original_priority        | null \n priority                 | null \n final_priority           | null \n als_unit                 | null \n call_type_group          | null \n number_of_alarms         | null \n unit_type                | null \n unit_sequence            | null \n Fire_prevention_district | null \n supervisor_district      | null \n Neighborhoods            | null \n row_id                   | null \n case_location            | null \n analysis_neighborhoods   | null \nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=49",
              "$$hashKey": "object:10193"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681552803372_1543210585",
      "id": "paragraph_1681552803372_1543210585",
      "dateCreated": "2023-04-15T10:00:03+0000",
      "dateStarted": "2023-04-19T20:14:54+0000",
      "dateFinished": "2023-04-19T20:14:55+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8500"
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType,DateType,TimestampType,BooleanType\n\nmy_schema = StructType([\n                                StructField(\"call_number\", StringType(), True),\n                                StructField(\"unit_id\",StringType(),True),\n                                StructField(\"incident_number\", IntegerType(),True),\n                                StructField(\"call_type\",StringType(),True),\n                                StructField(\"call_date\",DateType(),True),\n                                StructField(\"watch_date\",DateType(),True)  ,\n                                StructField(\"received_dtm\",TimestampType(),True),\n                                StructField(\"entry_dtm\",TimestampType(),True),\n                                StructField(\"dispatch_dtm\",TimestampType(),True),\n                                StructField(\"response_dtm\",TimestampType(),True),\n                                StructField(\"on_scene_dtm\",TimestampType(),True),\n                                StructField(\"transport_dtm\",TimestampType(),True),\n                                StructField(\"hospital_dtm\",TimestampType(),True),\n                                StructField(\"call_final_disposition\",StringType(),True),\n                                StructField(\"available_dtm\",TimestampType(),True),\n                                StructField(\"address\",StringType(),True),\n                                StructField(\"city\",StringType(),True),\n                                StructField(\"zipcode\",StringType(),True),\n                                StructField(\"battalion\",StringType(),True),\n                                StructField(\"station_area\",IntegerType(),True),\n                                StructField(\"box\",IntegerType(),True),\n                                StructField(\"original_priority\",IntegerType(),True),\n                                StructField(\"priority\",IntegerType(), True),\n                                StructField(\"final_priority\",IntegerType(),True),\n                                StructField(\"als_unit\",BooleanType(),True),\n                                StructField(\"call_type_group\",StringType(),True),\n                                StructField(\"number_of_alarms\",IntegerType(),True),\n                                StructField(\"unit_type\",StringType(),True),\n                                StructField(\"unit_sequence\",IntegerType(),True),\n                                StructField(\"Fire_prevention_district\",IntegerType(),True),\n                                StructField(\"supervisor_district\",IntegerType(),True),\n                                StructField(\"Neighborhoods\",StringType(),True),\n                                StructField(\"row_id\",StringType(),True),\n                                StructField(\"case_location\",StringType(),True),\n                                StructField(\"analysis_neighborhoods\",IntegerType(),True)  ] )\n                                \n                                \n                                \n                                \n       ",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T19:21:08+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681553252347_843181165",
      "id": "paragraph_1681553252347_843181165",
      "dateCreated": "2023-04-15T10:07:32+0000",
      "dateStarted": "2023-04-19T19:21:09+0000",
      "dateFinished": "2023-04-19T19:21:09+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8501"
    },
    {
      "text": "%pyspark\nfire_df=spark.read.\\\noption(\"header\",True). \\\nformat(\"csv\"). \\\nschema(my_schema). \\\nload(\"file:///data/fire.csv\")\nfire_df.printSchema()\n",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T19:17:58+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- call_number: integer (nullable = true)\n |-- unit_id: string (nullable = true)\n |-- incident_number: integer (nullable = true)\n |-- call_type: string (nullable = true)\n |-- call_date: date (nullable = true)\n |-- watch_date: date (nullable = true)\n |-- received_dtm: timestamp (nullable = true)\n |-- entry_dtm: timestamp (nullable = true)\n |-- dispatch_dtm: timestamp (nullable = true)\n |-- response_dtm: timestamp (nullable = true)\n |-- on_scene_dtm: timestamp (nullable = true)\n |-- transport_dtm: timestamp (nullable = true)\n |-- hospital_dtm: timestamp (nullable = true)\n |-- call_final_disposition: string (nullable = true)\n |-- available_dtm: timestamp (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- battalion: string (nullable = true)\n |-- station_area: integer (nullable = true)\n |-- box: integer (nullable = true)\n |-- original_priority: integer (nullable = true)\n |-- priority: integer (nullable = true)\n |-- final_priority: integer (nullable = true)\n |-- als_unit: boolean (nullable = true)\n |-- call_type_group: string (nullable = true)\n |-- number_of_alarms: integer (nullable = true)\n |-- unit_type: string (nullable = true)\n |-- unit_sequence: integer (nullable = true)\n |-- Fire_prevention_district: integer (nullable = true)\n |-- supervisor_district: integer (nullable = true)\n |-- Neighborhoods: string (nullable = true)\n |-- row_id: string (nullable = true)\n |-- case_location: string (nullable = true)\n |-- analysis_neighborhoods: integer (nullable = true)\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681553163784_769159658",
      "id": "paragraph_1681553163784_769159658",
      "dateCreated": "2023-04-15T10:06:03+0000",
      "dateStarted": "2023-04-19T19:17:58+0000",
      "dateFinished": "2023-04-19T19:17:58+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8502"
    },
    {
      "text": "%pyspark\nfire_df.show(4)",
      "user": "anonymous",
      "dateUpdated": "2023-04-19T19:18:11+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+-------+---------------+---------+---------+----------+------------+---------+------------+------------+------------+-------------+------------+----------------------+-------------+-------+----+-------+---------+------------+----+-----------------+--------+--------------+--------+---------------+----------------+---------+-------------+------------------------+-------------------+-------------+------+-------------+----------------------+\n|call_number|unit_id|incident_number|call_type|call_date|watch_date|received_dtm|entry_dtm|dispatch_dtm|response_dtm|on_scene_dtm|transport_dtm|hospital_dtm|call_final_disposition|available_dtm|address|city|zipcode|battalion|station_area| box|original_priority|priority|final_priority|als_unit|call_type_group|number_of_alarms|unit_type|unit_sequence|Fire_prevention_district|supervisor_district|Neighborhoods|row_id|case_location|analysis_neighborhoods|\n+-----------+-------+---------------+---------+---------+----------+------------+---------+------------+------------+------------+-------------+------------+----------------------+-------------+-------+----+-------+---------+------------+----+-----------------+--------+--------------+--------+---------------+----------------+---------+-------------+------------------------+-------------------+-------------+------+-------------+----------------------+\n|       null|   null|           null|     null|     null|      null|        null|     null|        null|        null|        null|         null|        null|                  null|         null|   null|null|   null|     null|        null|null|             null|    null|          null|    null|           null|            null|     null|         null|                    null|               null|         null|  null|         null|                  null|\n|       null|   null|           null|     null|     null|      null|        null|     null|        null|        null|        null|         null|        null|                  null|         null|   null|null|   null|     null|        null|null|             null|    null|          null|    null|           null|            null|     null|         null|                    null|               null|         null|  null|         null|                  null|\n|       null|   null|           null|     null|     null|      null|        null|     null|        null|        null|        null|         null|        null|                  null|         null|   null|null|   null|     null|        null|null|             null|    null|          null|    null|           null|            null|     null|         null|                    null|               null|         null|  null|         null|                  null|\n|       null|   null|           null|     null|     null|      null|        null|     null|        null|        null|        null|         null|        null|                  null|         null|   null|null|   null|     null|        null|null|             null|    null|          null|    null|           null|            null|     null|         null|                    null|               null|         null|  null|         null|                  null|\n+-----------+-------+---------------+---------+---------+----------+------------+---------+------------+------------+------------+-------------+------------+----------------------+-------------+-------+----+-------+---------+------------+----+-----------------+--------+--------------+--------+---------------+----------------+---------+-------------+------------------------+-------------------+-------------+------+-------------+----------------------+\nonly showing top 4 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=29",
              "$$hashKey": "object:10345"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681597737005_1198604362",
      "id": "paragraph_1681597737005_1198604362",
      "dateCreated": "2023-04-15T22:28:57+0000",
      "dateStarted": "2023-04-19T19:18:11+0000",
      "dateFinished": "2023-04-19T19:18:13+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8503"
    },
    {
      "text": "%pyspark\ntempDF2=spark.read.option(\"header\",False).format(\"csv\").option(\"skipRows\",1).option(\"mode\",\"DROPMALFORMED\").option(\"inferSchema\",False).load(\"/fire.csv\")",
      "user": "anonymous",
      "dateUpdated": "2023-04-15T22:45:40+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=96",
              "$$hashKey": "object:10393"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681565644259_1666356854",
      "id": "paragraph_1681565644259_1666356854",
      "dateCreated": "2023-04-15T13:34:04+0000",
      "dateStarted": "2023-04-15T22:45:40+0000",
      "dateFinished": "2023-04-15T22:45:40+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8504"
    },
    {
      "text": "%pyspark\nd = tempDF2.drop()",
      "user": "anonymous",
      "dateUpdated": "2023-04-15T22:45:40+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681566433416_1146555161",
      "id": "paragraph_1681566433416_1146555161",
      "dateCreated": "2023-04-15T13:47:13+0000",
      "dateStarted": "2023-04-15T22:45:40+0000",
      "dateFinished": "2023-04-15T22:45:40+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8505"
    },
    {
      "text": "%pyspark\ntempDF2.show(3)",
      "user": "anonymous",
      "dateUpdated": "2023-04-15T22:45:40+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+-------+---------------+------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-------------+--------------------+--------------------+--------------------+-------------+-------------------+---------+------------+----+-----------------+--------+--------------+--------+---------------+----------------+---------+--------------------+--------------------+-------------------+--------------------+-------------+--------------------+--------------------+\n|        _c0|    _c1|            _c2|         _c3|       _c4|       _c5|                 _c6|                 _c7|                 _c8|                 _c9|                _c10|          _c11|         _c12|                _c13|                _c14|                _c15|         _c16|               _c17|     _c18|        _c19|_c20|             _c21|    _c22|          _c23|    _c24|           _c25|            _c26|     _c27|                _c28|                _c29|               _c30|                _c31|         _c32|                _c33|                _c34|\n+-----------+-------+---------------+------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-------------+--------------------+--------------------+--------------------+-------------+-------------------+---------+------------+----+-----------------+--------+--------------+--------+---------------+----------------+---------+--------------------+--------------------+-------------------+--------------------+-------------+--------------------+--------------------+\n|Call Number|Unit ID|Incident Number|   Call Type| Call Date|Watch Date|       Received DtTm|          Entry DtTm|       Dispatch DtTm|       Response DtTm|       On Scene DtTm|Transport DtTm|Hospital DtTm|Call Final Dispos...|      Available DtTm|             Address|         City|Zipcode of Incident|Battalion|Station Area| Box|Original Priority|Priority|Final Priority|ALS Unit|Call Type Group|Number of Alarms|Unit Type|Unit sequence in ...|Fire Prevention D...|Supervisor District|Neighborhooods - ...|        RowID|       case_location|Analysis Neighbor...|\n|  221210313|    E36|       22054955|Outside Fire|05/01/2022|04/30/2022|05/01/2022 02:58:...|05/01/2022 02:59:...|05/01/2022 02:59:...|05/01/2022 03:01:...|05/01/2022 03:02:...|          null|         null|                Fire|05/01/2022 03:05:...|   GOUGH ST/GROVE ST|San Francisco|              94102|      B02|          36|3265|                3|       3|             3|    true|           Fire|               1|   ENGINE|                   1|                   2|                  5|        Hayes Valley|221210313-E36|POINT (-122.42316...|                   9|\n|  220190150|    E29|       22008871|      Alarms|01/19/2022|01/18/2022|01/19/2022 01:42:...|01/19/2022 01:44:...|01/19/2022 01:44:...|01/19/2022 01:46:...|01/19/2022 01:49:...|          null|         null|                Fire|01/19/2022 02:35:...|100 Block of MISS...|San Francisco|              94107|      B03|          29|2431|                3|       3|             3|    true|          Alarm|               1|   ENGINE|                   1|                   3|                 10|        Potrero Hill|220190150-E29|POINT (-122.39469...|                  26|\n+-----------+-------+---------------+------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+-------------+--------------------+--------------------+--------------------+-------------+-------------------+---------+------------+----+-----------------+--------+--------------+--------+---------------+----------------+---------+--------------------+--------------------+-------------------+--------------------+-------------+--------------------+--------------------+\nonly showing top 3 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=97",
              "$$hashKey": "object:10493"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681565579506_1716650052",
      "id": "paragraph_1681565579506_1716650052",
      "dateCreated": "2023-04-15T13:32:59+0000",
      "dateStarted": "2023-04-15T22:45:40+0000",
      "dateFinished": "2023-04-15T22:45:41+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8506"
    },
    {
      "text": "%pyspark\ntempDF2.toDF()",
      "user": "anonymous",
      "dateUpdated": "2023-04-15T22:45:41+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Fail to execute line 2: tempDF2.toDF()\nTraceback (most recent call last):\n  File \"/usr/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o1293.toDF.\n: java.lang.IllegalArgumentException: requirement failed: The number of columns doesn't match.\nOld column names (35): _c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c20, _c21, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33, _c34\nNew column names (0): \n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.sql.Dataset.toDF(Dataset.scala:447)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/1681558369119-0/zeppelin_python.py\", line 158, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/spark/python/pyspark/sql/dataframe.py\", line 2062, in toDF\n    jdf = self._jdf.toDF(self._jseq(cols))\n  File \"/usr/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/spark/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\npyspark.sql.utils.IllegalArgumentException: \"requirement failed: The number of columns doesn't match.\\nOld column names (35): _c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c20, _c21, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33, _c34\\nNew column names (0): \"\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681566173476_1196710924",
      "id": "paragraph_1681566173476_1196710924",
      "dateCreated": "2023-04-15T13:42:53+0000",
      "dateStarted": "2023-04-15T22:45:41+0000",
      "dateFinished": "2023-04-15T22:45:41+0000",
      "status": "ERROR",
      "$$hashKey": "object:8507"
    },
    {
      "text": "%pyspark\nfire_df.show(2,vertical=True)",
      "user": "anonymous",
      "dateUpdated": "2023-04-15T13:22:25+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-RECORD 0----------------------------------------------------\n Call Number                          | 221210313            \n Unit ID                              | E36                  \n Incident Number                      | 22054955             \n Call Type                            | Outside Fire         \n Call Date                            | 05/01/2022           \n Watch Date                           | 04/30/2022           \n Received DtTm                        | 05/01/2022 02:58:... \n Entry DtTm                           | 05/01/2022 02:59:... \n Dispatch DtTm                        | 05/01/2022 02:59:... \n Response DtTm                        | 05/01/2022 03:01:... \n On Scene DtTm                        | 05/01/2022 03:02:... \n Transport DtTm                       | null                 \n Hospital DtTm                        | null                 \n Call Final Disposition               | Fire                 \n Available DtTm                       | 05/01/2022 03:05:... \n Address                              | GOUGH ST/GROVE ST    \n City                                 | San Francisco        \n Zipcode of Incident                  | 94102                \n Battalion                            | B02                  \n Station Area                         | 36                   \n Box                                  | 3265                 \n Original Priority                    | 3                    \n Priority                             | 3                    \n Final Priority                       | 3                    \n ALS Unit                             | true                 \n Call Type Group                      | Fire                 \n Number of Alarms                     | 1                    \n Unit Type                            | ENGINE               \n Unit sequence in call dispatch       | 1                    \n Fire Prevention District             | 2                    \n Supervisor District                  | 5                    \n Neighborhooods - Analysis Boundaries | Hayes Valley         \n RowID                                | 221210313-E36        \n case_location                        | POINT (-122.42316... \n Analysis Neighborhoods               | 9                    \n-RECORD 1----------------------------------------------------\n Call Number                          | 220190150            \n Unit ID                              | E29                  \n Incident Number                      | 22008871             \n Call Type                            | Alarms               \n Call Date                            | 01/19/2022           \n Watch Date                           | 01/18/2022           \n Received DtTm                        | 01/19/2022 01:42:... \n Entry DtTm                           | 01/19/2022 01:44:... \n Dispatch DtTm                        | 01/19/2022 01:44:... \n Response DtTm                        | 01/19/2022 01:46:... \n On Scene DtTm                        | 01/19/2022 01:49:... \n Transport DtTm                       | null                 \n Hospital DtTm                        | null                 \n Call Final Disposition               | Fire                 \n Available DtTm                       | 01/19/2022 02:35:... \n Address                              | 100 Block of MISS... \n City                                 | San Francisco        \n Zipcode of Incident                  | 94107                \n Battalion                            | B03                  \n Station Area                         | 29                   \n Box                                  | 2431                 \n Original Priority                    | 3                    \n Priority                             | 3                    \n Final Priority                       | 3                    \n ALS Unit                             | true                 \n Call Type Group                      | Alarm                \n Number of Alarms                     | 1                    \n Unit Type                            | ENGINE               \n Unit sequence in call dispatch       | 1                    \n Fire Prevention District             | 3                    \n Supervisor District                  | 10                   \n Neighborhooods - Analysis Boundaries | Potrero Hill         \n RowID                                | 220190150-E29        \n case_location                        | POINT (-122.39469... \n Analysis Neighborhoods               | 26                   \nonly showing top 2 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id=60",
              "$$hashKey": "object:10603"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681564028838_694075319",
      "id": "paragraph_1681564028838_694075319",
      "dateCreated": "2023-04-15T13:07:08+0000",
      "dateStarted": "2023-04-15T13:22:25+0000",
      "dateFinished": "2023-04-15T13:22:26+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8508"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2023-04-16T01:51:35+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1681553193041_964278345",
      "id": "paragraph_1681553193041_964278345",
      "dateCreated": "2023-04-15T10:06:33+0000",
      "status": "FINISHED",
      "$$hashKey": "object:8509"
    }
  ],
  "name": "Test",
  "id": "2HYPHV9VE",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Test"
}